{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommylouistaylor/CEGE0004_MachineLearning/blob/master/6%20-%20Week/model_ensembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Ensembles"
      ],
      "metadata": {
        "id": "il8b6ytgVXV7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "MK7-FPJrSk_t"
      },
      "source": [
        "### Metadata: Breast Cancer Wisconsin Dataset\n",
        "\n",
        "This [dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) was computed from a digitized images of a breast mass and describes characteristics of the cell nuclei.\n",
        "\n",
        "### Metadata\n",
        "* 569 rows and 30 cols;\n",
        "* classic and easy binary classification dataset\n",
        "* Of 569 examples, 212 (xs) classified as malignant (y1) and 357 as benign (y2)\n",
        "\n",
        "### Attributes\n",
        "1. An id;\n",
        "1. Output Target (ys): the diagnosis (M = malignant, B = benign);\n",
        "1. Input Feature Examples (xs): 32 real-valued features\n",
        "    * radius (mean distance from center to perimeter);\n",
        "    * texture (standard deviation of gray-scale values);\n",
        "    * perimeter;\n",
        "    * area;\n",
        "    * smoothness (local variation in radius lengths);\n",
        "    * compactness (perimeter^2 / area - 1.0);\n",
        "    * concavity (severity of concave portions of the contour);\n",
        "    * concave points (number of concave portions of the contour);\n",
        "    * symmetry;\n",
        "    * fractal dimension (\"coastline approximation\" - 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "8Fx0GwRlta7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset using scikit-learn function"
      ],
      "metadata": {
        "id": "-3EsFNFViZQz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# read dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "dataset = load_breast_cancer()  # load dataset\n",
        "xs = dataset.data               # pass examples to var\n",
        "ys = dataset.target             # pass classes to var\n",
        "print('The dimensionality of the dataset is', xs.shape[1])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UsY9TG7oSk_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "xs  # view examples array"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "88hM5QZqSk_v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "ys  # view classes array"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ek5s7_NPSk_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing: Split into Training and Test sets"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "E5dK8j4bSk_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "# split into Training and Test\n",
        "from sklearn.model_selection import train_test_split\n",
        "xs_train, xs_test, ys_train, ys_test = train_test_split(xs, ys, test_size=0.20, random_state=42)  # 80% for the training set and 20% for the test set."
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MxOQ2b6eSk_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "# perform min-max scaling (since all vars are continuous)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()               # call scaler\n",
        "scaler.fit(xs_train)                  # fit training examples\n",
        "xs_train = scaler.transform(xs_train)\n",
        "xs_test = scaler.transform(xs_test)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "W6yfD5S-Sk_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stack Base Classifiers into a 'Stacked Classifier' using a Voting Mechanism"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "j04vPNsuSk_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define 4 Different Classifiers"
      ],
      "metadata": {
        "id": "64LUPcVqkP1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "# define classifiers\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "svm_clf = SVC(C=0.1)              # defines svc classifier\n",
        "nb_clf = MultinomialNB()          # defines Naive Bayes classifier\n",
        "dt_clf = DecisionTreeClassifier() # defines decision tree classifier\n",
        "mlp_clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)  # defines neural network classifer"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wjfgJZ0LSk_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Voting Classifier using scikit-learn"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "b64aBchVSk_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": [
        "# define voting classifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "voting_clf = VotingClassifier( \n",
        "    estimators=[('svm', svm_clf), ('nb', nb_clf), ('df', dt_clf), ('mlp', mlp_clf)],  # assign an id for each classifier\n",
        "    voting='hard')  # assign voting parameters: HARD=predicts class lavel based on majority rule voting, SOFT=predicts class label based on argmax of the sums of the predicted probabilities"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KgfVs2kwSk_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# fit voting classifier\n",
        "voting_clf.fit(xs_train, ys_train)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xT0RyNxeSk_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": [
        "# fit each individual classifier independenly\n",
        "for clf in (svm_clf, nb_clf, dt_clf, mlp_clf):\n",
        "    clf.fit(xs_train, ys_train)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JwEjAS-XSk_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measure Accuracy of ALL Classifier"
      ],
      "metadata": {
        "id": "jICN5s2MoX6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# measure train and test accuracy of each classifier, INCLUDING the ensembled classifier.\n",
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (svm_clf, nb_clf, dt_clf, mlp_clf, voting_clf):\n",
        "    print(clf.__class__.__name__)\n",
        "    ys_pred = clf.predict(xs_train)\n",
        "    print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
        "    ys_pred = clf.predict(xs_test)\n",
        "    print('\\ttest:', accuracy_score(ys_test, ys_pred))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iH3J6ft-Sk_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Bagging Classifier\n",
        "\n",
        "Bagging classifier ~ an ensemble meta-estimator. It fits base classifiers on random subsets of the original dataset and then aggregates their individual predictions (by voting or averaging) to form a final prediction."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "x2ZjXIr_Sk_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Bagging Classifier"
      ],
      "metadata": {
        "id": "1tcU81S0qaA0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "source": [
        "# perform bagging on a kNN classifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "bag_clf = BaggingClassifier(KNeighborsClassifier(),             # pass knn classifier into bagging classifier\n",
        "                            max_samples=0.5, max_features=1.0)  # max_samples and max_features control fraction of examples and features in each replica of the dataset"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "u3osdfaSSk_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# fit bagging classifier to sets\n",
        "bag_clf.fit(xs_train, ys_train)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "porUeSN-Sk_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# measure train and test accuracy of bagging classifier\n",
        "print(bag_clf.__class__.__name__, '(kNN)')\n",
        "ys_pred = bag_clf.predict(xs_train)\n",
        "print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
        "ys_pred = bag_clf.predict(xs_test)\n",
        "print('\\ttest:', accuracy_score(ys_test, ys_pred))"
      ],
      "metadata": {
        "id": "XK4qdW-5rNP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit Multiple Decision Tree Classifiers using 'Random Forest' Ensamble Model\n",
        "\n",
        "Random forest fits multiple decision tree classifiers on various sub-samples of\n",
        "the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True`, otherwise the whole dataset is used to build each tree."
      ],
      "metadata": {
        "collapsed": false,
        "id": "19FKzh7USk_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "source": [
        "# define random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=10)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UOKDCvJ6Sk_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# fit random forest classifier\n",
        "rf_clf.fit(xs_train, ys_train)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bQBPYEktSk_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate accuracy on train and test sets\n",
        "print(rf_clf.__class__.__name__)\n",
        "ys_pred = rf_clf.predict(xs_train)\n",
        "print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
        "ys_pred = rf_clf.predict(xs_test)\n",
        "print('\\ttest:', accuracy_score(ys_test, ys_pred))"
      ],
      "metadata": {
        "id": "zaIeT5QDs3ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting (AdaBoost)\n",
        "\n",
        "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then\n",
        "fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances\n",
        "are adjusted such that subsequent classifiers focus more on difficult cases.\n",
        "\n",
        "For this classifier we will boost 200 times decision trees of depth 1, aka decision stumps."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PDRj710ZSk_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0y4KjLipSk_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameter `n_estimators` controls the maximum number of estimators at which boosting is terminated.\n",
        "Of course, in case of a perfect fit, the learning procedure is stopped earlier.\n",
        "\n",
        "We now fit this classifier and evaluate its accuracy on the train and test sets."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "bwm5QmkRSk_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_clf.fit(xs_train, ys_train)\n",
        "\n",
        "print(ada_clf.__class__.__name__, '(DecisionStumps)')\n",
        "ys_pred = ada_clf.predict(xs_train)\n",
        "print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
        "ys_pred = ada_clf.predict(xs_test)\n",
        "print('\\ttest:', accuracy_score(ys_test, ys_pred))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4PCq2lgoSk_1",
        "outputId": "bc9d04ab-f6f4-4637-9017-db85fae975d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoostClassifier (DecisionStumps)\n",
            "\ttrain: 1.0\n",
            "\ttest: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in all the examples above we have not performed any validation.\n",
        "How would you perform the validation of these classifiers?"
      ],
      "metadata": {
        "collapsed": false,
        "id": "HYLAFOZxSk_1"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "model_ensembles.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}